# hsv_person_tracker.py
import cv2
import numpy as np
from ultralytics import YOLO
import os
from datetime import datetime
import time

from hand_gesture import HandGestureRecognizer
from frame_grabber import FrameGrabber
import tcp_client   # ìƒˆë¡œ ë¶„ë¦¬í•œ TCP ëª¨ë“ˆ

# Qt í”Œë«í¼ í”ŒëŸ¬ê·¸ì¸ ì˜¤ë¥˜ ë°©ì§€
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['QT_AUTO_SCREEN_SCALE_FACTOR'] = '0'
os.environ['QT_SCALE_FACTOR'] = '1'


# --- HSV ê¸°ë°˜ ReID + ì œìŠ¤ì²˜ ì¸ì‹ ---
class HSVAnalyzer:
    def __init__(self):
        # YOLO ì„¸ê·¸ë©˜í…Œì´ì…˜(ì‚¬ëŒ í´ë˜ìŠ¤)
        self.model = YOLO('yolov8s-seg.pt')

        # ì‚¬ëŒë³„ HSV íˆìŠ¤í† ê·¸ë¨ / bbox ì €ì¥
        # people_data[pid] = {'histograms': [np.array], 'bboxes': [(x1,y1,x2,y2), ...]}
        self.people_data = {}
        self.next_id = 0

        # ë§¤ì¹­ ê´€ë ¨ ì„¤ì •
        self.match_threshold = 0.35           # ReID ì ìˆ˜ ì„ê³„
        self.min_detection_confidence = 0.6   # YOLO confidence
        self.min_person_area = 5000           # ë„ˆë¬´ ì‘ì€ ì‚¬ëŒì€ ë¬´ì‹œ
        self.max_histograms_per_person = 13   # í•œ ì‚¬ëŒë‹¹ íˆìŠ¤í† ê·¸ë¨ ê¸°ì–µ ê°œìˆ˜

        # ì œìŠ¤ì²˜ ì¸ì‹ê¸° (MediaPipe ìœ ì§€)
        self.gesture = HandGestureRecognizer(
            max_hands=1, det_conf=0.6, track_conf=0.5,
            smooth_window=5, hold_frames=3, process_every_n=3
        )
        # ì‚¬ëŒë³„ ì† ì œìŠ¤ì²˜ ìƒíƒœ
        # person_action_state[pid] = {'gesture': str, 'last_action': str, 'ts': float}
        self.person_action_state = {}

        # ì œìŠ¤ì²˜ ì²˜ë¦¬ ì¡°ê±´
        self.upper_body_ratio = 0.6       # bbox ìƒë‹¨ 60%ë§Œ ìƒë°˜ì‹  ROI
        self.gesture_every_n_frames = 3   # ì œìŠ¤ì²˜ ì¶”ë¡  ì£¼ê¸°
        self._gesture_frame_mod = 0

        # ì œìŠ¤ì²˜ ì¸ì‹ ëŒ€ìƒìœ¼ë¡œ ê³ ì •í•  ì‚¬ëŒ ID
        self.target_person_id = "Person_0"

        # Person_0 í…œí”Œë¦¿(EMA)
        self.target_template = {
            "hist": None,   # np.ndarray(48,) - HSV íˆìŠ¤í† ê·¸ë¨
            "bbox": None,   # ë§ˆì§€ë§‰ bbox
        }
        self.template_alpha = 0.2          # EMA ê°€ì¤‘(0.1~0.3)
        self.template_match_boost = 0.15   # Person_0ì¼ ë•Œ ì ìˆ˜ ë³´ì •

        # í˜¸ì¶œ ìƒíƒœ
        self.call_state = False
        self.action = None

        self.recognition = True

    # --- íˆìŠ¤í† ê·¸ë¨ & ìœ ì‚¬ë„ ---
    def extract_histogram(self, img, mask, bins=16):
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        h_hist = cv2.calcHist([hsv], [0], mask, [bins], [0, 180])
        s_hist = cv2.calcHist([hsv], [1], mask, [bins], [0, 256])
        v_hist = cv2.calcHist([hsv], [2], mask, [bins], [0, 256])

        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()

        combined_hist = np.concatenate([h_hist, s_hist, v_hist])
        return combined_hist

    def calculate_similarity_metrics(self, hist1, hist2):
        h1 = hist1.astype(np.float32)
        h2 = hist2.astype(np.float32)

        bhatt_dist = cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA)
        dot_product = float(np.dot(h1, h2))
        norm1 = float(np.linalg.norm(h1))
        norm2 = float(np.linalg.norm(h2))
        cosine_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
        return bhatt_dist, cosine_sim

    def _hist_similarity(self, h1, h2):
        bhatt, cos = self.calculate_similarity_metrics(h1, h2)
        return max(1.0 - bhatt, cos)

    # --- Person_0 í…œí”Œë¦¿ ì—…ë°ì´íŠ¸ ---
    def _update_target_template(self, pid, hist, bbox):
        if pid != self.target_person_id:
            return
        if self.target_template["hist"] is None:
            self.target_template["hist"] = hist.copy()
        else:
            self.target_template["hist"] = (
                (1 - self.template_alpha) * self.target_template["hist"]
                + self.template_alpha * hist
            )
        self.target_template["bbox"] = bbox

    # --- ReID ë§¤ì¹­ ---
    def find_best_match(self, current_hist, current_bbox, used_ids):
        best_match_id = None
        best_score = 0.0

        x1, y1, x2, y2 = current_bbox
        current_center = ((x1 + x2) // 2, (y1 + y2) // 2)

        for pid, pdata in self.people_data.items():
            if pid in used_ids:
                continue
            if len(pdata['histograms']) == 0:
                continue

            # ìµœê·¼ íˆìŠ¤í† ê·¸ë¨ë“¤ ì¤‘ì—ì„œ ìµœê³  ìœ ì‚¬ë„
            hist_scores = []
            for stored_hist in pdata['histograms'][-self.max_histograms_per_person:]:
                sim = self._hist_similarity(current_hist, stored_hist)
                hist_scores.append(sim)
            best_hist_score = max(hist_scores) if hist_scores else 0.0

            # í™”ë©´ ìœ„ì¹˜(ì¤‘ì‹¬ì  ê±°ë¦¬) ì ìˆ˜
            latest_bbox = pdata['bboxes'][-1]
            sx1, sy1, sx2, sy2 = latest_bbox
            stored_center = ((sx1 + sx2) // 2, (sy1 + sy2) // 2)
            center_distance = np.sqrt(
                (current_center[0] - stored_center[0]) ** 2 +
                (current_center[1] - stored_center[1]) ** 2
            )
            max_distance = np.sqrt(640 ** 2 + 480 ** 2)
            spatial_score = 1.0 - (center_distance / max_distance)

            total_score = 0.9 * best_hist_score + 0.1 * spatial_score

            # Person_0ì´ë©´ í…œí”Œë¦¿ ìœ ì‚¬ë„ë¡œ ì¶”ê°€ ê°€ì‚°ì 
            if self.target_template["hist"] is not None and pid == self.target_person_id:
                template_sim = self._hist_similarity(current_hist, self.target_template["hist"])
                total_score += self.template_match_boost * template_sim

            if total_score > best_score:
                best_score = total_score
                best_match_id = pid

        return best_match_id, best_score

    # --- ë©”ì¸ ë£¨í”„ ---
    def run_analysis(self):
        grabber = FrameGrabber(src=0, width=960, height=540, fps=30, fourcc='MJPG').start()
        if grabber.read() is None:
            print("âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨")
            grabber.stop()
            return

        frame_count = 0
        send = 0
        start_time = datetime.now()
        frame_skip_counter = 0
        process_every_n_frames = 3

        try:
            while True:
                frame = grabber.read()
                if frame is None:
                    time.sleep(0.005)
                    continue

                frame_count += 1
                elapsed_time = (datetime.now() - start_time).total_seconds()

                # í”„ë ˆì„ ìŠ¤í‚µìœ¼ë¡œ YOLO ë¶€í•˜ ê°ì†Œ
                if frame_skip_counter < process_every_n_frames - 1:
                    frame_skip_counter += 1
                    continue
                frame_skip_counter = 0

                annotated = frame.copy()

                # --- ì‚¬ëŒ ê°ì§€ (ì„¸ê·¸ë©˜í…Œì´ì…˜) ---
                results = self.model(annotated, classes=[0], imgsz=480, verbose=False)
                current_detections = []

                for result in results:
                    if result.masks is None or result.boxes is None:
                        continue

                    masks = result.masks.data
                    boxes = result.boxes

                    for i in range(len(boxes)):
                        box = boxes[i]
                        confidence = float(box.conf[0].item())
                        if confidence < self.min_detection_confidence:
                            continue

                        seg = masks[i]
                        mask = seg.cpu().numpy().astype(np.uint8) * 255
                        mask_resized = cv2.resize(
                            mask,
                            (frame.shape[1], frame.shape[0]),
                            interpolation=cv2.INTER_NEAREST
                        )

                        # ëª¨í´ë¡œì§€: í° ëŒ€ìƒë§Œ í•œ ë²ˆ ì •ë¦¬
                        person_pixels = cv2.countNonZero(mask_resized)
                        if person_pixels > 3000:
                            kernel = np.ones((3, 3), np.uint8)
                            mask_cleaned = cv2.morphologyEx(
                                mask_resized, cv2.MORPH_CLOSE, kernel, iterations=1
                            )
                        else:
                            mask_cleaned = mask_resized

                        # HSV íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
                        combined_hist = self.extract_histogram(frame, mask_cleaned)

                        bbox = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = bbox
                        area = (x2 - x1) * (y2 - y1)
                        if area < self.min_person_area:
                            continue

                        current_detections.append({
                            'hist': combined_hist,
                            'bbox': bbox,
                            'mask': mask_cleaned,
                            'confidence': confidence,
                            'area': area,
                        })

                # í° ì‚¬ëŒë¶€í„° ì²˜ë¦¬
                current_detections.sort(key=lambda x: x['area'], reverse=True)
                used_ids = set()

                for detection in current_detections:
                    combined_hist = detection['hist']
                    bbox = detection['bbox']

                    matched_id, match_score = self.find_best_match(
                        combined_hist, bbox, used_ids
                    )

                    # ë§¤ì¹­ ì„±ê³µ
                    if matched_id is not None and match_score > self.match_threshold:
                        pdata = self.people_data[matched_id]
                        pdata['histograms'].append(combined_hist)
                        pdata['bboxes'].append(bbox)
                        used_ids.add(matched_id)

                        # ì˜¤ë˜ëœ ê¸°ë¡ ì‚­ì œ
                        if len(pdata['histograms']) > self.max_histograms_per_person:
                            pdata['histograms'].pop(0)
                            pdata['bboxes'].pop(0)

                        pid = matched_id
                        color = (0, 255, 0)
                    else:
                        # ìƒˆ ì‚¬ëŒ ë“±ë¡
                        pid = f"Person_{self.next_id}"
                        self.people_data[pid] = {
                            'histograms': [combined_hist],
                            'bboxes': [bbox],
                        }

                        if self.recognition:
                            # ğŸ”¹ TCP ëª¨ë“ˆë¡œ ë¶„ë¦¬ëœ í•¨ìˆ˜ í˜¸ì¶œ
                            tcp_client.user_result()

                        self.recognition = False
                        self.next_id += 1
                        used_ids.add(pid)
                        color = (0, 0, 255)

                    # Person_0 í…œí”Œë¦¿ ì—…ë°ì´íŠ¸
                    self._update_target_template(pid, combined_hist, bbox)

                    x1, y1, x2, y2 = map(int, bbox)
                    cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(
                        annotated, pid,
                        (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2
                    )

                    # ---------- ì œìŠ¤ì²˜ ì¸ì‹ (Person_0ë§Œ) ----------
                    self._gesture_frame_mod = (self._gesture_frame_mod + 1) % self.gesture_every_n_frames
                    if pid != self.target_person_id:
                        continue  # íƒ€ê¹ƒì´ ì•„ë‹ˆë©´ ì œìŠ¤ì²˜ ìŠ¤í‚µ

                    do_gesture_now = (self._gesture_frame_mod == 0)
                    if do_gesture_now:
                        roi_top = y1
                        roi_bottom = y1 + int((y2 - y1) * self.upper_body_ratio)
                        roi_left, roi_right = x1, x2

                        roi_top = max(0, roi_top)
                        roi_bottom = min(frame.shape[0], roi_bottom)
                        roi_left = max(0, roi_left)
                        roi_right = min(frame.shape[1], roi_right)

                        upper_roi = annotated[roi_top:roi_bottom, roi_left:roi_right]

                        label = self.gesture.infer(upper_roi, pid)
                        if label:
                            prev = self.person_action_state.get(
                                pid, {'gesture': None, 'last_action': None, 'ts': 0.0}
                            )

                            # â‘  í˜¸ì¶œ ìƒíƒœê°€ ì•„ë‹Œë° 'FARTHEST'ì´ë©´ â†’ í˜¸ì¶œ ì‹œì‘
                            if not self.call_state and label == "Shaka":
                                self.action = "Call"
                                self.call_state = True
                                send = 1
                                print("[CALL MODE ON] í˜¸ì¶œë¨")

                            # â‘¡ í˜¸ì¶œ ìƒíƒœì¼ ë•Œë§Œ ë‹¤ë¥¸ ì œìŠ¤ì²˜ë¥¼ ìœ íš¨í•˜ê²Œ í•´ì„
                            if self.call_state:
                                if label == "OK_SIGN":
                                    self.action = "RESTART";   self.call_state = False
                                elif label == "V_SIGN":
                                    self.action = "FOLLOWING";   self.call_state = False
                                    send = 3;  tcp_client.send_detection_result(2, 3, send, 0, 0, 0)
                                elif label == "THREE":
                                    self.action = "GUIDING";     self.call_state = False
                                    send = 4;  tcp_client.send_detection_result(2, 3, send, 0, 0, 0)
                                elif label == "OPEN":
                                    self.action = "QUIT";      self.call_state = False
                                    send = 5;  tcp_client.send_detection_result(2, 3, send, 0, 0, 0)
                                elif label == "FIST":
                                    self.action = "STOP";      self.call_state = False
                                    send = 2;  tcp_client.send_detection_result(2, 3, send, 0, 0, 0)

                            new_gesture = label
                            new_action = self.action

                            if prev['gesture'] != new_gesture or prev['last_action'] != new_action:
                                self.person_action_state[pid] = {
                                    'gesture': new_gesture,
                                    'last_action': new_action,
                                    'ts': elapsed_time,
                                }

                    # --- í…ìŠ¤íŠ¸ í‘œì‹œ (í•­ìƒ ë§ˆì§€ë§‰ ìƒíƒœ ê¸°ì¤€) ---
                    state = self.person_action_state.get(pid)
                    if state and state.get('last_action') is not None:
                        cv2.putText(
                            annotated,
                            f"Hand: {state['gesture']}, CALL: {self.call_state}",
                            (x1, y1 - 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                            (0, 255, 255), 2
                        )
                        cv2.putText(
                            annotated,
                            f"Action: {state['last_action']}",
                            (x1, y1 - 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                            (0, 0, 255), 2
                        )

                # --- í™”ë©´ ì •ë³´ í‘œì‹œ ---
                info_text = f"People:{len(self.people_data)} | Frame:{frame_count} | Time:{elapsed_time:.1f}s"
                cv2.putText(
                    annotated, info_text,
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                    (0, 255, 255), 2
                )

                # í‘œì‹œ(ë¶€í•˜ ì¤„ì´ë ¤ë©´ resize í•´ì„œ ë³´ì—¬ì¤˜ë„ ë¨)
                if frame_count % 5 == 0:
                    cv2.imshow("HSV ReID + Person_0 Gesture", annotated)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

        finally:
            grabber.stop()
            cv2.destroyAllWindows()

        print(f"\nğŸ“Š ì¢…ë£Œ! ì‚¬ëŒ ìˆ˜: {len(self.people_data)}")


if __name__ == "__main__":
    # 1) TCP ì—°ê²° ìŠ¤ë ˆë“œ ì‹œì‘
    tcp_client.start_tcp_thread()

    # 2) ë¶„ì„ ì‹œì‘
    analyzer = HSVAnalyzer()
    analyzer.run_analysis()
